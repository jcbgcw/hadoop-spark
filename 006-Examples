//CLEAN UP
$
hive -e 'SHOW DATABASES' | xargs -I '{}' hive -e 'DROP DATABASE IF EXISTS {} CASCADE'
hadoop fs -rm -r -f /user/cloudera/*
hadoop fs -rm -r -f /user/hive/warehouse/*

Sqoop
================================================================================
Sqoop Hdfs Import
---------------------------------------
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_text --fields-terminated-by '\001';
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_text_snappy --fields-terminated-by '\001' --compression-codec snappy;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_text_gzip --fields-terminated-by '\001' --compression-codec gzip;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_avro --as-avrodatafile;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_avro_snappy --as-avrodatafile --compression-codec snappy;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_parquet --as-parquetfile;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_seqfile --as-sequencefile;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_seqfile_snappy --as-sequencefile --compression-codec snappy;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --target-dir /user/cloudera/sqoop_import_hdfs/orders_seqfile_gzip --as-sequencefile --compression-codec gzip;

Sqoop Hive Import
---------------------------------------
hive> create database if not exists sqoop_hive_import;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --hive-import --hive-database sqoop_hive_import --hive-table orders_text.orders;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --hive-import --hive-database sqoop_hive_import --hive-table orders_text_snappy --compression-codec snappy;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --hive-import --hive-database sqoop_hive_import --hive-table orders_text_gzip --compression-codec gzip;
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --table orders -m 1 --hive-import --hive-database sqoop_hive_import --hive-table orders_parquet --as-parquetfile;

sqoop import-all-tables --connect jdbc:mysql://quickstart:3306/retail_db --username retail_dba --password cloudera --hive-import --hive-database sqoop_import_all;

Hive
================================================================================

CREATE table with data from existing file
--------------------------------------------------------------------------------

create database if not exists hive_create_from_hdfs;
use hive_create_from_hdfs;

--TEXT

create external table orders_text
(ORDER_ID INT, ORDER_DATE STRING, ORDER_CUSTOMER_ID INT, ORDER_STATUS STRING)
location '/user/cloudera/sqoop_import_hdfs/orders_text_snappy';

select * from hive_create_from_hdfs.orders_text limit 1;

--SEQUENCE

--Sqoop Sequence SerDe and Hive Sequence SerDe is different.
--Thatâ€™s why we need to use Hive-Sqoop-SerDe, refer above link for reference
--https://questforthought.wordpress.com/2015/11/25/load-sqoop-sequence-files-in-hive/


--AVRO
$ hadoop fs -get /user/cloudera/sqoop_import_hdfs/orders_avro_snappy/part-m-00000.avro
$ avro-tools getschema part-m-00000.avro > orders.avsc
$ hadoop fs -mkdir /user/hive/schema
$ hadoop fs -put orders.avsc /user/hive/schema/orders.avsc

create external table orders_avro
    (order_id int, ORDER_DATE BIGINT, order_customer_id int, order_status string)
    stored as avro location '/user/cloudera/sqoop_import_hdfs/orders_avro_snappy';
select * from orders_avro limit 1;

create external table orders_avro_2 like orders_text
    stored as avro location '/user/cloudera/sqoop_import_hdfs/orders_avro_snappy';
alter table orders_avro_2 CHANGE ORDER_DATE ORDER_DATE BIGINT;
select * from orders_avro_2 limit 1;

create external table orders_avro_3
    stored as avro location '/user/cloudera/sqoop_import_hdfs/orders_avro_snappy'
    TBLPROPERTIES('avro.schema.url'='/user/hive/schema/orders.avsc');
select * from orders_avro_3 limit 1;

create external table orders_avro_4
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
    STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
              OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
    location '/user/cloudera/sqoop_import_hdfs/orders_avro_snappy'
    TBLPROPERTIES('avro.schema.url'='/user/hive/schema/orders.avsc');
select * from orders_avro_4 limit 1;

//PARQUET: not like text table where delimiters defined which causes error,
//should like avro table, thus avro.schema.utl is copied

create external table orders_parquet
    (order_id int, ORDER_DATE BIGINT, order_customer_id int, order_status string)
    stored as parquet location '/user/cloudera/sqoop_import_hdfs/orders_parquet';
select * from orders_parquet limit 1;

create external table orders_parquet_2 LIKE ORDERS_TEXT
    stored as parquet location '/user/cloudera/sqoop_import_hdfs/orders_parquet';
alter table orders_parquet_2 CHANGE ORDER_DATE ORDER_DATE BIGINT;
select * from orders_parquet_2 limit 1;

create external table orders_parquet_3 LIKE ORDERS_AVRO
    stored as parquet location '/user/cloudera/sqoop_import_hdfs/orders_parquet';
select * from orders_parquet_3 limit 1;

Create empty table with explicit column definitions
================================================================================

// LOAD DATA INPATH '/user/cloudera/orders_gzip' INTO TABLE orders_text_new;
// ONLY FILE COPY

$ hive -e "set;" | grep "compress"

create database hive_create_empty_for_insert;
use hive_create_empty_for_insert;

create table orders_text like hive_create_from_hdfs.orders_text;
insert into orders_text select * from hive_create_from_hdfs.orders_text;
select * from orders_text limit 1;

set hive.exec.compress.output=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
create table orders_text_snappy like orders_text;
insert into orders_text_snappy select * from orders_text;
select * from orders_text_snappy limit 1;
set hive.exec.compress.output=false;

create table orders_parquet like hive_create_from_hdfs.orders_parquet;
insert into orders_parquet select * from hive_create_from_hdfs.orders_parquet;
select * from orders_parquet limit 1;

set parquet.compression=snappy;
create table orders_parquet_snappy like orders_parquet;
insert into orders_parquet_snappy select * from orders_parquet;
select * from orders_parquet_snappy limit 1;

set parquet.compression=gzip;
create table orders_parquet_gzip like orders_parquet;
insert into orders_parquet_gzip select * from orders_parquet;
select * from orders_parquet_gzip limit 1;

set parquet.compression=uncompressed;
create table orders_parquet_uncompressed like orders_parquet;
insert into orders_parquet_uncompressed select * from orders_parquet;
select * from orders_parquet_uncompressed limit 1;

create table orders_seqfile like orders_parquet stored as sequencefile;
insert into orders_seqfile select * from orders_parquet;
select * from orders_seqfile limit 1;

set hive.exec.compress.output=true;
set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
create table orders_seqfile_snappy like orders_seqfile;
insert into orders_seqfile_snappy select * from orders_seqfile;
select * from orders_seqfile_snappy limit 1;
set hive.exec.compress.output=false;

create table orders_avro like hive_create_from_hdfs.orders_avro;
insert into orders_avro select * from hive_create_from_hdfs.orders_avro;
select * from orders_avro limit 1;

set hive.exec.compress.output=true;
set avro.output.codec=snappy;
create table orders_avro_snappy like orders_avro;
insert into orders_avro_snappy select * from orders_avro;
select * from orders_avro_snappy limit 1;
set hive.exec.compress.output=false;

create table orders_orc like orders_avro stored as orc;
insert into orders_orc select * from orders_avro;
select * from orders_orc_snappy limit 1;

set hive.exec.compress.output=true;
set orc.compress=SNAPPY;
create table orders_orc_snappy like orders_orc;
insert into orders_orc_snappy select * from orders_orc;
select * from orders_orc_snappy limit 1;

set hive.exec.compress.output=true;
set orc.compress=ZLIB;
create table orders_orc_zlib like orders_orc;
insert into orders_orc_zlib select * from orders_orc;
select * from orders_orc_zlib limit 1;

set hive.exec.compress.output=true;
set orc.compress=NONE;
create table orders_orc_none like orders_orc;
insert into orders_orc_none select * from orders_orc;
select * from orders_orc_none limit 1;

create table orders_orc_zlib2 like orders_orc tblproperties("orc.compress"="ZLIB");
insert into orders_orc_zlib2 select * from orders_orc;
select * from orders_orc_zlib2 limit 1;

create table orders_orc_none2 like orders_orc tblproperties("orc.compress"="NONE");
insert into orders_orc_none2 select * from orders_orc;
select * from orders_orc_none2 limit 1;

Create Table As Select (CTAS)
==============================
create database hive_create_as_select;
use hive_create_as_select;

//CREATE-TABLE-AS-SELECT cannot create external table

create table orders_parquet stored as parquet location '/user/cloudera/create-table-as-select/orders'
    as select * from hive_create_from_hdfs.orders_parquet;
