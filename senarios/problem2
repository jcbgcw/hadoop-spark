http://arun-teaches-u-tech.blogspot.sg/p/cca-175-prep-problem-scenario-2.html

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba --password cloudera \
--table products --target-dir  /user/cloudera/products \
--fields-terminated-by '|'

hadoop fs -mkdir problem2
hadoop fs -mv /user/cloudera/products /user/cloudera/problem2/products

hadoop fs -chmod 765 /user/cloudera/problem2/products/*

val rdd = sc.textFile("/user/cloudera/problem2/products/").map(_.split('|')).
    map(s=>(s(1).toInt, s(4).toFloat)).filter(_._2<=100)
val df = rdd.toDF("c","p")

val resultDf = df.groupBy("c").agg(max($"p"),count("p"),round(avg($"p"),2),min($"p")).orderBy("c").
    toDF("product_category","max_price","total_products","average_price","min_price").coalesce(1)
resultDf.show
import com.databricks.spark.avro._
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
resultDf.write.avro("/user/cloudera/problem2/products/result-df")

df.registerTempTable("t")
val resultSql = sqlContext.sql("""
    select c, max(p), count(1), CAST(AVG(P) AS DECIMAL(10,2)),min(p)
    from t group by c order by c""").
    toDF("product_category","max_price","total_products","average_price","min_price").coalesce(1)
resultSql.show
resultSql.write.avro("/user/cloudera/problem2/products/result-sql")

val resultRdd = rdd.aggregateByKey(0f,0,0f,9999f)((r,p)=> (r._1.max(p),r._2+1,r._3+p,r._4.min(p)), (r1,r2)=>(r1._1.max(r2._1),r1._2+r2._2,r1._3+r2._3,r1._4.min(r2._4))).
    map(s=>(s._1, s._2._1,s._2._2,"%.2f".format(s._2._3/s._2._2),s._2._4)).sortBy(_._1).
    toDF("product_category","max_price","total_products","average_price","min_price").coalesce(1)
resultRdd.show
resultRdd.write.avro("/user/cloudera/problem2/products/result-rdd")

sqlContext.setConf("spark.sql.avro.compression.codec","uncompressed")
resultRdd.write.avro("/user/cloudera/problem2/products/result-rdd-un")