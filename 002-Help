Sqoop
===============================================================================
$sqoop help
$sqoop help merge
$sqoop merge --help

HDFS Commands
===============================================================================
$ hadoop
$ hadoop fs
$ hadoop fs -help ls
$ hadoop fs -usage ls

Hive
===============================================================================
hive> show functions;
hive> show functions like '*date*|*time*';
hive> describe function
hive> describe function from_unixtime;
hive> describe function extended from_unixtime;

Spark
===============================================================================
$ spark-submit --help
$ spark-shell --help
$ pyspark --help

Scala
===============================================================================
scala> :help
scala> :history
scala> :imports
 1) import scala.Predef._          (162 terms, 78 are implicit)
 2) import org.apache.spark.SparkContext._ (83 terms, 7 are implicit)
 3) import sqlContext.implicits._  (60 terms, 36 are implicit)
 4) import sqlContext.sql          (2 terms)
 5) import org.apache.spark.sql.functions._ (300 terms)

scala> :implicits
/* 7 implicit members imported from org.apache.spark.SparkContext */
  /* 7 defined in org.apache.spark.SparkContext */
  implicit def boolToBoolWritable(b: Boolean): org.apache.hadoop.io.BooleanWritable
  implicit def bytesToBytesWritable(aob: Array[Byte]): org.apache.hadoop.io.BytesWritable
  implicit def doubleToDoubleWritable(d: Double): org.apache.hadoop.io.DoubleWritable
  implicit def floatToFloatWritable(f: Float): org.apache.hadoop.io.FloatWritable
  implicit def intToIntWritable(i: Int): org.apache.hadoop.io.IntWritable
  implicit def longToLongWritable(l: Long): org.apache.hadoop.io.LongWritable
  implicit def stringToText(s: String): org.apache.hadoop.io.Text


/* 36 implicit members imported from org.apache.spark.sql.SQLContext$implicits */
  /* 1 defined in org.apache.spark.sql.SQLContext$implicits */
  implicit def StringToColumn(sc: StringContext): org.apache.spark.sql.SQLContext.implicits.StringToColumn

  /* 35 inherited from org.apache.spark.sql.SQLImplicits */
  implicit def intRddToDataFrameHolder(data: org.apache.spark.rdd.RDD[Int]): org.apache.spark.sql.DataFrameHolder
  implicit def localSeqToDataFrameHolder[A <: Product](data: Seq[A])(implicit evidence$7: reflect.runtime.universe.TypeTag[A]): org.apache.spark.sql.DataFrameHolder
  implicit def longRddToDataFrameHolder(data: org.apache.spark.rdd.RDD[Long]): org.apache.spark.sql.DataFrameHolder
  implicit def rddToDataFrameHolder[A <: Product](rdd: org.apache.spark.rdd.RDD[A])(implicit evidence$6: reflect.runtime.universe.TypeTag[A]): org.apache.spark.sql.DataFrameHolder
  implicit def stringRddToDataFrameHolder(data: org.apache.spark.rdd.RDD[String]): org.apache.spark.sql.DataFrameHolder

  implicit def newBooleanArrayEncoder: org.apache.spark.sql.Encoder[Array[Boolean]]
  implicit def newBooleanEncoder: org.apache.spark.sql.Encoder[Boolean]
  implicit def newBooleanSeqEncoder: org.apache.spark.sql.Encoder[Seq[Boolean]]
  implicit def newByteArrayEncoder: org.apache.spark.sql.Encoder[Array[Byte]]
  implicit def newByteEncoder: org.apache.spark.sql.Encoder[Byte]
  implicit def newByteSeqEncoder: org.apache.spark.sql.Encoder[Seq[Byte]]
  implicit def newDoubleArrayEncoder: org.apache.spark.sql.Encoder[Array[Double]]
  implicit def newDoubleEncoder: org.apache.spark.sql.Encoder[Double]
  implicit def newDoubleSeqEncoder: org.apache.spark.sql.Encoder[Seq[Double]]
  implicit def newFloatArrayEncoder: org.apache.spark.sql.Encoder[Array[Float]]
  implicit def newFloatEncoder: org.apache.spark.sql.Encoder[Float]
  implicit def newFloatSeqEncoder: org.apache.spark.sql.Encoder[Seq[Float]]
  implicit def newIntArrayEncoder: org.apache.spark.sql.Encoder[Array[Int]]
  implicit def newIntEncoder: org.apache.spark.sql.Encoder[Int]
  implicit def newIntSeqEncoder: org.apache.spark.sql.Encoder[Seq[Int]]
  implicit def newLongArrayEncoder: org.apache.spark.sql.Encoder[Array[Long]]
  implicit def newLongEncoder: org.apache.spark.sql.Encoder[Long]
  implicit def newLongSeqEncoder: org.apache.spark.sql.Encoder[Seq[Long]]
  implicit def newProductArrayEncoder[A <: Product](implicit evidence$3: reflect.runtime.universe.TypeTag[A]): org.apache.spark.sql.Encoder[Array[A]]
  implicit def newProductEncoder[T <: Product](implicit evidence$1: reflect.runtime.universe.TypeTag[T]): org.apache.spark.sql.Encoder[T]
  implicit def newProductSeqEncoder[A <: Product](implicit evidence$2: reflect.runtime.universe.TypeTag[A]): org.apache.spark.sql.Encoder[Seq[A]]
  implicit def newShortArrayEncoder: org.apache.spark.sql.Encoder[Array[Short]]
  implicit def newShortEncoder: org.apache.spark.sql.Encoder[Short]
  implicit def newShortSeqEncoder: org.apache.spark.sql.Encoder[Seq[Short]]
  implicit def newStringArrayEncoder: org.apache.spark.sql.Encoder[Array[String]]
  implicit def newStringEncoder: org.apache.spark.sql.Encoder[String]
  implicit def newStringSeqEncoder: org.apache.spark.sql.Encoder[Seq[String]]

  implicit def localSeqToDatasetHolder[T](s: Seq[T])(implicit evidence$5: org.apache.spark.sql.Encoder[T]): org.apache.spark.sql.DatasetHolder[T]
  implicit def rddToDatasetHolder[T](rdd: org.apache.spark.rdd.RDD[T])(implicit evidence$4: org.apache.spark.sql.Encoder[T]): org.apache.spark.sql.DatasetHolder[T]
  implicit def symbolToColumn(s: Symbol): org.apache.spark.sql.ColumnName

scala> sc.getClass
scala> :reset
scala> :quit

Python
===============================================================================
>>> help
help> modules
help> module spark
>>> help("modules")
>>> help("modules spark")
>>> help("modules accu")
>>> help("pyspark.accumulators")
>>> help(rdd)
>>> help(sc)
>>> help(sqlContext)
>>> dir(sc)[36:]

Flume
===============================================================================
$ flume-ng help

Pig
===============================================================================
$ pig -help
grunt> help

MySql
===============================================================================
mysql> help;
mysql> help show;

